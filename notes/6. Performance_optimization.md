# Performance Optimization
## Overview
- It makes run query faster and saves the cost
- Traditional ways of optimization which snowflake manages automatically
  - Add indexes and PK
  - Create table partitions
  - Analyse the query execution plan
  - Remove unnecessary full table scans
- SF automatically manages micro partitions, sizing virtual Warehouses(VWH) and clusterkeys
- OPtimizations ways
  - Dedicated VWH: for different workloads for different groups of peoples
  - Scaling UP: for known patterns of high workload
  - Scaling Out: for unknown patterns of workloads
  - Maximize Cache: automatic cache can be maximized
  - Cluster Keys: for large tables,

# Create dedicated virtual warehouses
- identify and classify different team in org with different workload needs. Ex. BI, DS, Mktg
- for every class of workload create dedicated VWH & assign users
- Avoid too many VWHs, to avoid underutilization
- Refine classification of workload or user groups needs time to time

# Implement dedicated VWH
```
//  Create virtual warehouse for data scientist & DBA
// Data Scientists
CREATE WAREHOUSE DS_WH 
WITH
WAREHOUSE_SIZE = 'SMALL'
WAREHOUSE_TYPE = 'STANDARD' 
AUTO_SUSPEND = 300 
AUTO_RESUME = TRUE 
MIN_CLUSTER_COUNT = 1 
MAX_CLUSTER_COUNT = 1 
SCALING_POLICY = 'STANDARD';

// DBA
CREATE WAREHOUSE DBA_WH 
WITH
WAREHOUSE_SIZE = 'XSMALL'
WAREHOUSE_TYPE = 'STANDARD' 
AUTO_SUSPEND = 300 
AUTO_RESUME = TRUE 
MIN_CLUSTER_COUNT = 1 
MAX_CLUSTER_COUNT = 1 
SCALING_POLICY = 'STANDARD';

// Create role for Data Scientists & DBAs
CREATE ROLE DATA_SCIENTIST;
GRANT USAGE ON WAREHOUSE DS_WH TO ROLE DATA_SCIENTIST;
CREATE ROLE DBA;
GRANT USAGE ON WAREHOUSE DBA_WH TO ROLE DBA;


// Setting up users with roles 
// Data Scientists 
CREATE USER DS1 PASSWORD = 'DS1' LOGIN_NAME = 'DS1' DEFAULT_ROLE='DATA_SCIENTIST' DEFAULT_WAREHOUSE = 'DS_WH'  MUST_CHANGE_PASSWORD = FALSE;
CREATE USER DS2 PASSWORD = 'DS2' LOGIN_NAME = 'DS2' DEFAULT_ROLE='DATA_SCIENTIST' DEFAULT_WAREHOUSE = 'DS_WH'  MUST_CHANGE_PASSWORD = FALSE;
CREATE USER DS3 PASSWORD = 'DS3' LOGIN_NAME = 'DS3' DEFAULT_ROLE='DATA_SCIENTIST' DEFAULT_WAREHOUSE = 'DS_WH'  MUST_CHANGE_PASSWORD = FALSE;

GRANT ROLE DATA_SCIENTIST TO USER DS1;
GRANT ROLE DATA_SCIENTIST TO USER DS2;
GRANT ROLE DATA_SCIENTIST TO USER DS3;

// DBAs
CREATE USER DBA1 PASSWORD = 'DBA1' LOGIN_NAME = 'DBA1' DEFAULT_ROLE='DBA' DEFAULT_WAREHOUSE = 'DBA_WH'  MUST_CHANGE_PASSWORD = FALSE;
CREATE USER DBA2 PASSWORD = 'DBA2' LOGIN_NAME = 'DBA2' DEFAULT_ROLE='DBA' DEFAULT_WAREHOUSE = 'DBA_WH'  MUST_CHANGE_PASSWORD = FALSE;

GRANT ROLE DBA TO USER DBA1;
GRANT ROLE DBA TO USER DBA2;

// ---Try login using above credentials and check which WH is accessible

// Drop objects at the end
DROP USER DBA1;
DROP USER DBA2;
DROP USER DS1;
DROP USER DS2;
DROP USER DS3;
DROP ROLE DATA_SCIENTIST;
DROP ROLE DBA;
DROP WAREHOUSE DS_WH;
DROP WAREHOUSE DBA_WH;
```
## Scaling up/down
- Change size of VWH as during different workload times of the day. LIke ETL runs at specific time or business event
- scaling up/down is suitable in case of query complexity NOT more users
- We can use ALTER WAREHOUSE query to set size of warehouse same can be done from admin console

## Scaling out
- means using additional warehouses or multicluster warehouses. We can automate this scaling process
- It's suitable when there are more concurrent users/queries
Considerations:
- at least Enterprise edition all WH should be multicluster

```
// We will run below query in 8to9 to worksheets say 8to9 concurruntely to see how cluster get spin up when the load is increased and again snips down when the load is released
// We can check WH cluster count under admin->WH console
/ Once cluster snips up suspend WH and all queries to save credits

SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.WEB_SITE T1
CROSS JOIN SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.WEB_SITE T2
CROSS JOIN SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.WEB_SITE T3
CROSS JOIN (SELECT TOP 57 * FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.WEB_SITE)  T4
```
## Caching
- This automatical process to speed up the queries
- IF query is executed twice, results are cached and can be re-used
- Results are cached for 24 hours or until underlying data had changed
- To ensure that caching happens make sure that those queries should be running on same WH

```
// Query first time takes 2.1 sec and second time 110ms as first-time table scan take longer time to scan entire table with 200mill records 
SELECT AVG(C_BIRTH_YEAR) FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.CUSTOMER

// Setting up an additional user and try to run the above query under same WH will utilize the cache and return result faster
CREATE ROLE DATA_SCIENTIST;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE DATA_SCIENTIST;
CREATE USER DS1 PASSWORD = 'DS1' LOGIN_NAME = 'DS1' DEFAULT_ROLE='DATA_SCIENTIST' DEFAULT_WAREHOUSE = 'DS_WH'  MUST_CHANGE_PASSWORD = FALSE;
GRANT ROLE DATA_SCIENTIST TO USER DS1;
// Try running the same query with a new user and under the same WH. Observe the execution time.

//Drop table at the end
DROP ROLE DATA_SCIENTIST;
DROP USER 

```

## Cluster key
- Clustering is subset of rows to locate the data n
```
// Publicly accessible staging area    

CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage
    url='s3://bucketsnowflakes3';

// List files in stage

LIST @MANAGE_DB.external_stages.aws_stage;

//Load data using copy command

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*OrderDetails.*';
    

// Create table

CREATE OR REPLACE TABLE ORDERS_CACHING (
ORDER_ID	VARCHAR(30)
,AMOUNT	NUMBER(38,0)
,PROFIT	NUMBER(38,0)
,QUANTITY	NUMBER(38,0)
,CATEGORY	VARCHAR(30)
,SUBCATEGORY	VARCHAR(30)
,DATE DATE)    



INSERT INTO ORDERS_CACHING 
SELECT
t1.ORDER_ID
,t1.AMOUNT	
,t1.PROFIT	
,t1.QUANTITY	
,t1.CATEGORY	
,t1.SUBCATEGORY	
,DATE(UNIFORM(1500000000,1700000000,(RANDOM())))
FROM ORDERS t1
CROSS JOIN (SELECT * FROM ORDERS) t2
CROSS JOIN (SELECT TOP 100 * FROM ORDERS) t3


// Query Performance before Cluster Key

SELECT * FROM ORDERS_CACHING  WHERE DATE = '2020-06-09'


// Adding Cluster Key & Compare the result

ALTER TABLE ORDERS_CACHING CLUSTER BY ( DATE ) 

SELECT * FROM ORDERS_CACHING  WHERE DATE = '2020-01-05'


// Not ideal clustering & adding a different Cluster Key using function

SELECT * FROM ORDERS_CACHING  WHERE MONTH(DATE)=11

ALTER TABLE ORDERS_CACHING CLUSTER BY ( MONTH(DATE) )

```
