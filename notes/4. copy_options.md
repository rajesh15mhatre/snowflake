 # Copy Options
 ## Validation
 - Validation Mode: while copying data we can use this mode to validate data **it will not load the data**. below are the different types.
  - validation_mode=RETURN_ERRORS : this option will show errors from the entire file.
  - validation_mode=RETURN_N_ROWS : this will validate the data of the first N rows and will show the data.
```
---- VALIDATION_MODE ----
// Prepare database & table
CREATE OR REPLACE DATABASE COPY_DB;
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object 
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
// LIST files in S3
LIST @COPY_DB.PUBLIC.aws_stage_copy;

---- Use files without errors ----
 
//Load data using copy command with RETURN_ERROR validation mode which will return error row details like column, row, file, name and type of error and data will
// not be loaded in table.
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS

//Check if data loaded in the table
SEELCT * FROM ORDERS;

// This will validate first 5 rows and data will not be loaded
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
   VALIDATION_MODE = RETURN_5_ROWS 

---- Use files with errors ----
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/returnfailed/';

LIST @COPY_DB.PUBLIC.aws_stage_copy;    

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS;

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Error.*'
    VALIDATION_MODE = RETURN_1_rows;
```
## Rejected records: 
we can check the rejected records
     - we can get rejected data using below methods
     validate(orders, job_id => '_last')
```
-------------- Working with error results -----------

---- 1) Saving rejected files with VALIDATION_MODE ---- 
// Create table
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Validate files
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS;

// Storing rejected/failed results in a new table and insert rejected records
// IMP-this will store entire error row under one column
// last_query_id() will give ID of the last ran query. We can also get this ID manually from right side of the output windows under "query details" section.
CREATE OR REPLACE TABLE rejected AS 
select rejected_record from table(result_scan(last_query_id()));

INSERT INTO rejected
select rejected_record from table(result_scan(last_query_id()));

SELECT * FROM rejected;

---- 2) Saving rejected files skipping VALIDATION_MODE ----
//BElow query will load correct records and will skip error rows
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    ON_ERROR=CONTINUE;
// select error rows from last query
select * from table(validate(orders, job_id => '_last'));

---- 3) loading rejected records into table under proper column ---- 
SELECT REJECTED_RECORD FROM rejected;

CREATE OR REPLACE TABLE rejected_values as
SELECT 
SPLIT_PART(rejected_record,',',1) as ORDER_ID, 
SPLIT_PART(rejected_record,',',2) as AMOUNT, 
SPLIT_PART(rejected_record,',',3) as PROFIT, 
SPLIT_PART(rejected_record,',',4) as QUATNTITY, 
SPLIT_PART(rejected_record,',',5) as CATEGORY, 
SPLIT_PART(rejected_record,',',6) as SUBCATEGORY
FROM rejected; 

SELECT * FROM rejected_values;
```
## Size Limit

- We can specify maximum file size limits in bytes in COPY command
- When the limit is exceeded, the COPY operation stops loading the next files
 - SIZE_LIMIT=20000
```
---- SIZE_LIMIT ----
// Prepare database & table
CREATE OR REPLACE DATABASE COPY_DB;
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));
    
// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
        
// List files in stage
LIST @aws_stage_copy;

//Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    SIZE_LIMIT=20000;
```

## Failed only
- Specifies whether to return only failed files in result. Default is false.
 - RETURN_FAILED_ONLY = TRUE |FALSE
- we use it with ON_ERROR = CONTINUE to get error only files
```
---- RETURN_FAILED_ONLY ----
// Create table
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/returnfailed/';
LIST @COPY_DB.PUBLIC.aws_stage_copy
  
 //Load data using copy command with return fail only true  will show error as it will get error in error files
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    RETURN_FAILED_ONLY = TRUE
// load data using "ON_ERROR = continue" and "RETURN_failed_only = true". This will show only failed files results as "ON_ERROR=continue" will ignore the errors
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    ON_ERROR =CONTINUE
    RETURN_FAILED_ONLY = TRUE

// Default RETURN_FAILED_ONLY is FALSE this it will show all error and normal files
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    ON_ERROR =CONTINUE
```

## Truncate Columns
- Specifies whether to truncate text string that exceed the target column length which we specifies while defining table Ex. AMOUNT VARCHAR(30) 
 -  TRUNCATECOLUMNS = true will truncate the string value and false(default) will raise an error 
```
---- TRUNCATECOLUMNS ----
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(10),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
LIST @COPY_DB.PUBLIC.aws_stage_copy;
      
//Load data using copy command
//  This will show error as Category column has value more that 10 in length
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'

// Trying to load again with "truncatecolumn = true" so it wil truncate values as per defined length and no error will be shown
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    TRUNCATECOLUMNS = true; 
// Check results
SELECT * FROM ORDERS;    
```

## Force 
- Specifies to load all files, regardless of whether they've been loaded previously and have not changed since they were loaded
 - FORCE = TRUE
- this option reloads data and potentially duplicates data in table.
```
---- FORCE ----
CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
LIST @COPY_DB.PUBLIC.aws_stage_copy
  
 //Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'

// Not possible to load file that have been loaded and data has not been modified so it will be skipped
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
//check data
SELECT * FROM ORDERS;    

//Load data using the FORCE option
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    FORCE = TRUE;
```

## Load history
- enables you to retrieve the history of data loaded into tables using COPY INTO <table> command
- you can find this information under snowflake->account_usage->Views->load_history
```

-- Query load history within a database --

USE COPY_DB;
// Shows copy command usage log under specific schema
SELECT * FROM information_schema.load_history

-- Query load history gloabally from SNOWFLAKE database --
SELECT * FROM snowflake.account_usage.load_history


// Filter on specific table & schema
SELECT * FROM snowflake.account_usage.load_history
  where schema_name='PUBLIC' and
  table_name='ORDERS'
  
  
// Filter on specific table & schema
SELECT * FROM snowflake.account_usage.load_history
  where schema_name='PUBLIC' and
  table_name='ORDERS' and
  error_count > 0
  
  
// Filter on specific table & schema
SELECT * FROM snowflake.account_usage.load_history
WHERE DATE(LAST_LOAD_TIME) <= DATEADD(days,-1,CURRENT_DATE)

```
